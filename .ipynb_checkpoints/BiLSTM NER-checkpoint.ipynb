{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "torch.cuda.set_device(1)\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('used_data-Copy1.p', 'rb') as handle:\n",
    "    data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =data[:100]\n",
    "\n",
    "#load X and y\n",
    "x =[[j[0] for j in i] for i in text]\n",
    "y =[[j[1] for j in i] for i in text]\n",
    "\n",
    "#unique words and tags\n",
    "total_vocab =list(set([j[0]for i in text for j in i]))\n",
    "total_tags = [ i for i in range(6)]\n",
    "\n",
    "x =list(filter(None,x))\n",
    "y= list(filter(None,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "vocab['PAD'] = 0\n",
    "vocab['UNK'] = 1\n",
    "#start at 1 becuase we add UNK at 0 index\n",
    "for i,l in enumerate(total_vocab,2):\n",
    "    vocab[l] =i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter UNK word from datasets\n",
    "train_sentences = []\n",
    "train_labels=[]\n",
    "for sentence in x:\n",
    "    s= [ vocab[token] if token in vocab else vocab['UNK'] for token in sentence]\n",
    "    train_sentences.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels =y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_lengths = torch.cuda.LongTensor(list(map(len, train_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maximum len of seq\n",
    "max_len = max(len(s) for s in train_sentences)\n",
    "# get the length of each seq in your batch\n",
    "seq_lengths = torch.LongTensor(list(map(len, train_sentences)))\n",
    "# seq_lengths = np.stack(list(map(len, train_sentences)))\n",
    "\n",
    "\n",
    "seq_tensor = Variable(torch.zeros((len(train_sentences), seq_lengths.max()))).long()\n",
    "for idx, (seq, seqlen) in enumerate(zip(train_sentences, seq_lengths)):\n",
    "    seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "    \n",
    "labels_tensor = Variable(torch.zeros((len(train_sentences), seq_lengths.max()))).long()\n",
    "for idx, (label, seqlen) in enumerate(zip(y, seq_lengths)):\n",
    "    labels_tensor[idx, :seqlen] = torch.LongTensor(label)\n",
    "\n",
    "# SORT YOUR TENSORS BY LENGTH!\n",
    "seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "seq_tensor = seq_tensor[perm_idx]\n",
    "#we didnt transpose because we need batch in beginning\n",
    "# seq_tensor = seq_tensor.transpose(0,1)\n",
    "labels = labels_tensor[perm_idx]\n",
    "# labels = labels.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_valid(x,y,percent):\n",
    "    percent = percent/100\n",
    "    x_train =(x[:round(len(x)*(percent))])\n",
    "    y_train =(x[:round(len(x)*(1-percent))])\n",
    "    x_test =(y[:round(len(y)*(percent))])\n",
    "    y_test =(y[:round(len(y)*(1-percent))])\n",
    "    return x_train,x_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =create_valid(seq_tensor,labels,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([90, 322]),\n",
       " torch.Size([90, 322]),\n",
       " torch.Size([10, 322]),\n",
       " torch.Size([10, 322]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 322])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperpramater\n",
    "bs =512\n",
    "hiddem_size = 64\n",
    "emb_size = 100\n",
    "lr = 1e-3\n",
    "num_classes = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trn_dataset(Dataset):\n",
    "        def __init__(self):\n",
    "            self.len = seq_tensor.shape[0]\n",
    "            self.x_data = seq_tensor\n",
    "            self.y_data = labels\n",
    "            self.seq_len = seq_lengths\n",
    "            self.perm_idx =perm_idx\n",
    "            \n",
    "        def __getitem__(self,index):\n",
    "            return self.x_data[index],self.y_data[index],self.seq_len[index],self.perm_idx[index]\n",
    "        def __len__(self):\n",
    "            return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class valid_dataset(Dataset):\n",
    "        def __init__(self):\n",
    "            self.len = seq_tensor.shape[0]\n",
    "            self.x_data = seq_tensor\n",
    "            self.y_data = labels\n",
    "            self.seq_len = seq_lengths\n",
    "            self.perm_idx =perm_idx\n",
    "            \n",
    "        def __getitem__(self,index):\n",
    "            return self.x_data[index],self.y_data[index],self.seq_len[index],self.perm_idx[index]\n",
    "        def __len__(self):\n",
    "            return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader =  DataLoader(dataset=trn_dataset(),batch_size=bs,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(lstm,self).__init__()\n",
    "        #layer\n",
    "        self.embedding = nn.Embedding(len(vocab),emb_size)\n",
    "        self.lstm =  nn.LSTM(emb_size,hiddem_size, bidirectional=True)\n",
    "        self.fc = nn.Linear(hiddem_size*2,num_classes)\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self,bs=bs):\n",
    "        self.h = (Variable(torch.zeros(2, bs, hiddem_size).cuda()),Variable(torch.zeros(2, bs, hiddem_size).cuda()))\n",
    "    \n",
    "    \n",
    "    def forward(self,inp,seq_lengths):\n",
    "        bs = inp[0].size(0)\n",
    "        if self.h[0].size() != bs:\n",
    "            self.init_hidden(bs)\n",
    "\n",
    "        emb = self.embedding(inp)\n",
    "\n",
    "        packed_input = pack_padded_sequence(emb, seq_lengths.numpy())\n",
    "        packed_output,h = self.lstm(packed_input,self.h)\n",
    "        self.h = h\n",
    "        out, _ = pad_packed_sequence(packed_output)\n",
    "        s = self.fc(out)\n",
    "        return F.log_softmax(s,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lstm(\n",
       "  (embedding): Embedding(699, 100)\n",
       "  (lstm): LSTM(100, 64, bidirectional=True)\n",
       "  (fc): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lstm()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(inp,targ):\n",
    "    sl,bs,nh = inp.size()\n",
    "    #we want targ size match the inp size because we use pack pad only in x but y \n",
    "    targ = targ[:,:sl]\n",
    "    targ = targ.transpose(0,1).contiguous().view(-1)\n",
    "    \n",
    "    return F.nll_loss(inp.view(-1,nh),targ.view(-1))\n",
    "#     return F.nll_loss(inp.view(-1,nh),targ.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def div(denominator,numerator):\n",
    "    if numerator == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        result = denominator/numerator\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(precision,recall):\n",
    "    try:\n",
    "        \n",
    "        return 2*((precision*recall)/(precision+recall))\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(m,epoch):\n",
    "    number_classes = int(m.shape[0])\n",
    "    clear_output(wait=True)\n",
    "    print('Epoch: {} Time: {}'.format(epoch,timeSince(start)))\n",
    "    \n",
    "    for i in range(number_classes):\n",
    "#     for i in range(6):\n",
    "#         if i >=number_classes:\n",
    "\n",
    "#             print('Class: {} P: {:.2f}, R: {:.2f}, F1: {:.2f}'.format(i,0,0,0))\n",
    "#             continue\n",
    "        precsion = div(m[i,i],sum(m[:,i]))\n",
    "        recall = div(m[i,i],sum(m[i,:]))\n",
    "        f1_score = f1(precsion,recall)\n",
    "        \n",
    "        print('Class: {} P: {:.2f}, R: {:.2f}, F1: {:.2f}'.format(i,precsion,recall,f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def concat_bs_result(outputs,p):\n",
    "    \n",
    "    outc =outputs.transpose(0,1)\n",
    "    all_preds = []\n",
    "    all_acts = []\n",
    "    for i,(data,p_idx) in enumerate(zip(outc,p)):\n",
    "        t = np.argmax(data.cpu().detach().numpy(),axis=1)\n",
    "        max_len_act =len(np.array(train_labels[p_idx]))\n",
    "        pred = list(t[:max_len_act])\n",
    "        act =list(np.array(train_labels[p_idx]))\n",
    "        all_preds.extend(pred)\n",
    "        all_acts.extend(act)\n",
    "    return all_preds,all_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer =optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Time: 0m 0s\n",
      "Class: 0 P: 0.94, R: 0.01, F1: 0.02\n",
      "Class: 1 P: 0.00, R: 0.00, F1: nan\n",
      "Class: 2 P: 0.00, R: 0.43, F1: 0.01\n",
      "Class: 3 P: 0.02, R: 0.17, F1: 0.03\n",
      "Class: 4 P: 0.02, R: 0.63, F1: 0.03\n",
      "Class: 5 P: 0.00, R: 0.00, F1: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bobbyl2k/miniconda3/envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "all_loss =[]\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "    all_p=[]\n",
    "    all_a=[]\n",
    "    all_loss=0.0\n",
    "\n",
    "    for index,data in enumerate(train_loader):\n",
    "        \n",
    "        numbers = index+1\n",
    "        x,y,l,p = data\n",
    "        x,y,l,p = Variable(x).cuda(), Variable(y).cuda(),Variable(l),Variable(p)\n",
    "        x = x.transpose(0,1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs =model(x,l)\n",
    "        loss = nll_loss_seq(outputs,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        all_loss += loss.item()\n",
    "        if numbers%10==0:\n",
    "            print('Avg.loss: {:.4f}, Time :{} '.format(all_loss/numbers,timeSince(start)))\n",
    "#         all_loss.append(loss.item())\n",
    "\n",
    "\n",
    "        all_p_minib,all_a_minib =concat_bs_result(outputs)\n",
    "        all_p.extend(all_p_minib)\n",
    "        all_a.extend(all_a_minib)\n",
    "    m =confusion_matrix(all_a, all_p)\n",
    "    evaluate(m,epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
