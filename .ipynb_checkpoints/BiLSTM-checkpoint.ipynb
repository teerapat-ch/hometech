{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gensim\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "modelwv = Doc2Vec.load(\"./model/doc2vec_before_train.wv\")\n",
    "#embedding = from_pretrained(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "----\n",
    "\n",
    "Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "userLog = pd.read_csv(\"./clean_data/userlog_feature.csv\")\n",
    "userLog['datetime'] = pd.to_datetime(userLog['datetime'])\n",
    "isTrain = userLog['datetime']<datetime.datetime(2018,2,18)\n",
    "test = userLog[~isTrain]\n",
    "userLog = userLog[isTrain]\n",
    "userLog1 = userLog.sort_values(\"datetime\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "# What projects has this user been clicked.\n",
    "userLogSeq = userLog1.groupby(\"userCode\").agg({\"project_id\":lambda x: list(x)})\n",
    "#userLogSeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out filter that we only click once in this database. (We'll use them later.)\n",
    "userLogSeqFiltered = userLogSeq[userLogSeq['project_id'].apply(lambda x: len(x))>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map project id to project Index [0 to index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1311"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelwv.wv.index2word.index('6709')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/u5722792744_1/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "userLogSeqFiltered2 = userLogSeqFiltered\n",
    "userLogSeqFiltered2['project_id'] = userLogSeqFiltered['project_id'].apply(lambda x: [modelwv.wv.index2word.index(str(i)) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/u5722792744_1/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/home/users/u5722792744_1/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data format\n",
    "[12, 14, 15, 17, 14, 17, 12], -> [12,14,15,16,14,17] : [14, 15, 17, 14, 17, 12]\n",
    "\"\"\"\n",
    "userLogSeqFiltered2['features'] = userLogSeqFiltered2['project_id'].apply(lambda x: x[:-1])\n",
    "userLogSeqFiltered2['targets'] = userLogSeqFiltered2['project_id'].apply(lambda x: x[1:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(169721, 4)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare into pad pack sequence\n",
    "userLogSeqFiltered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/u5722792744_1/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/users/u5722792744_1/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Pad and save length as function\n",
    "userLogSeqFiltered['seq_len'] = userLogSeqFiltered['features'].apply(len)\n",
    "userSeq = userLogSeqFiltered.sort_values('seq_len',ascending=False)[userLogSeqFiltered['seq_len']<5000]\n",
    "maxSeq = max(userSeq['seq_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad with 0\n",
    "userSeq['padded_features'] = userSeq.apply(lambda x: x['features']+[0]*(maxSeq - x['seq_len']),axis=1)\n",
    "#userSeq['padded_targets'] = userSeq.apply(lambda x: x['targets']+[0]*(maxSeq - x['seq_len']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([169719, 1640])\n",
      "torch.Size([169719, 1640])\n",
      "torch.Size([169719])\n"
     ]
    }
   ],
   "source": [
    "features = torch.Tensor(userSeq['padded_features'])\n",
    "targets = torch.Tensor(userSeq['padded_targets'])\n",
    "seq_len = torch.Tensor(userSeq['seq_len'].values)\n",
    "\n",
    "print(features.shape)\n",
    "print(targets.shape)\n",
    "print(seq_len.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PadPack: (seq_len, ) #ต้องเรียงความยาวแล้ว"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/u5722792744_1/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       " 4.9195e-01 -1.3319e-01 -3.0650e-01  ...   1.2673e-01 -5.1500e-01 -1.8527e-01\n",
       " 5.6054e-02 -1.9014e-02 -4.6964e-01  ...   1.6318e-01 -4.0418e-01 -3.7207e-01\n",
       " 4.4348e-01 -9.8378e-02 -3.0109e-01  ...  -1.2436e-01 -3.5961e-01 -1.6795e-01\n",
       "                ...                   ⋱                   ...                \n",
       " 1.9528e-04 -3.2744e-04 -5.4844e-04  ...   6.4551e-04  1.5027e-03 -2.9178e-04\n",
       " 8.1053e-03  1.5074e-02  4.2196e-03  ...  -5.8520e-03  1.0764e-02 -4.1962e-03\n",
       " 7.7862e-03  1.9477e-02 -1.0397e-03  ...  -1.0143e-02  8.4091e-03  5.6552e-04\n",
       "[torch.FloatTensor of size 5317x300]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.FloatTensor(modelwv.wv.syn0)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "hidden_size = 500\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        #self.projEmbedding = nn.Embedding(*weights.shape).from_pretrained(weight) # Spreading tuple into embedding dim\n",
    "        self.projEmbedding = from_pretrained(weights)\n",
    "        \n",
    "        self.lstm = nn.LSTM(weights.shape[1],hidden_size, bidirectional = False, dropout = 0.5)\n",
    "        \n",
    "        self.hidden2out = nn.Linear(hidden_size, 300)\n",
    "        \n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(p=0.5)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #self.init_hidden()\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        self.hidden = (Variable(torch.zeros(1, bs, hidden_size)),Variable(torch.zeros(1, bs, hidden_size)))\n",
    "        \n",
    "    def forward(self, bs, lengths):\n",
    "\n",
    "        \n",
    "        embeds = self.projEmbedding(bs)\n",
    "        \n",
    "        transposed_embeds = embeds.transpose(0,1)\n",
    "        \n",
    "        self.init_hidden(5)\n",
    "        \n",
    "        packed_input = pack_padded_sequence(transposed_embeds, lengths.numpy())\n",
    "        packed_outputs, (ht, ct) = self.lstm(packed_input, self.hidden)\n",
    "        \n",
    "        out, _ = pad_packed_sequence(packed_outputs)\n",
    "        \n",
    "        out = self.hidden2out(out)\n",
    "        \n",
    "        return out.transpose(0,1)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_pretrained(embeddings, freeze=True):\n",
    "    assert embeddings.dim() == 2, \\\n",
    "         'Embeddings parameter is expected to be 2-dimensional'\n",
    "    rows, cols = embeddings.shape\n",
    "    embedding = torch.nn.Embedding(num_embeddings=rows, embedding_dim=cols)\n",
    "    embedding.weight = torch.nn.Parameter(embeddings)\n",
    "    embedding.weight.requires_grad = not freeze\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0  ,.,.) = \n",
       " -5.6728e-03 -3.7201e-02 -1.5633e-02  ...  -1.6681e-02 -1.2741e-03  9.3813e-03\n",
       " -4.2244e-02 -5.3611e-02 -3.5277e-02  ...  -7.9317e-03 -1.8351e-03  1.9747e-02\n",
       " -6.0949e-02 -6.1588e-02 -4.4063e-02  ...  -3.2549e-03 -1.1087e-03  2.3199e-02\n",
       "                 ...                   ⋱                   ...                \n",
       " -2.4071e-02 -3.7844e-02 -4.7426e-02  ...  -9.2498e-03  4.9147e-03  3.1097e-02\n",
       " -2.7360e-02 -4.3730e-02 -5.4155e-02  ...  -1.0072e-02 -5.7113e-05  3.2012e-02\n",
       " -1.7761e-02 -4.7859e-02 -3.3893e-02  ...  -9.2280e-03  4.8992e-03  2.9285e-02\n",
       "\n",
       "( 1  ,.,.) = \n",
       " -2.1136e-02 -6.6315e-02 -2.1001e-03  ...  -1.5239e-03 -2.1452e-02  2.5345e-02\n",
       " -3.0316e-02 -7.9940e-02  4.2396e-03  ...   8.0847e-03 -3.1422e-02  3.0659e-02\n",
       " -5.9645e-02 -5.3712e-02 -2.5233e-02  ...  -4.4178e-02 -3.5385e-02 -1.0573e-03\n",
       "                 ...                   ⋱                   ...                \n",
       " -6.4433e-03 -3.9881e-02 -1.4241e-02  ...  -1.7338e-02  6.7836e-03  1.5492e-02\n",
       " -6.4433e-03 -3.9881e-02 -1.4241e-02  ...  -1.7338e-02  6.7836e-03  1.5492e-02\n",
       " -6.4433e-03 -3.9881e-02 -1.4241e-02  ...  -1.7338e-02  6.7836e-03  1.5492e-02\n",
       "\n",
       "( 2  ,.,.) = \n",
       " -1.1524e-02 -3.2192e-02 -1.0906e-02  ...  -9.5124e-03 -1.6137e-03  1.4032e-02\n",
       " -3.7120e-02 -7.3866e-03 -2.7002e-02  ...  -2.1540e-03  6.2533e-03 -8.8047e-03\n",
       " -4.6613e-02 -3.7755e-04 -2.9231e-02  ...  -1.5074e-03  1.2085e-03 -1.3336e-02\n",
       "                 ...                   ⋱                   ...                \n",
       " -6.4433e-03 -3.9881e-02 -1.4241e-02  ...  -1.7338e-02  6.7836e-03  1.5492e-02\n",
       " -6.4433e-03 -3.9881e-02 -1.4241e-02  ...  -1.7338e-02  6.7836e-03  1.5492e-02\n",
       " -6.4433e-03 -3.9881e-02 -1.4241e-02  ...  -1.7338e-02  6.7836e-03  1.5492e-02\n",
       "\n",
       "( 3  ,.,.) = \n",
       "  1.7811e-02 -5.2086e-02 -2.6217e-02  ...  -9.4577e-03 -2.1016e-02  1.4773e-02\n",
       "  3.0967e-02 -5.7001e-02 -2.8527e-02  ...  -5.5560e-03 -3.6854e-02  1.2994e-02\n",
       "  3.8119e-02 -5.8609e-02 -2.7824e-02  ...  -3.3265e-03 -4.5713e-02  1.1731e-02\n",
       "                 ...                   ⋱                   ...                \n",
       " -6.4433e-03 -3.9881e-02 -1.4241e-02  ...  -1.7338e-02  6.7836e-03  1.5492e-02\n",
       " -6.4433e-03 -3.9881e-02 -1.4241e-02  ...  -1.7338e-02  6.7836e-03  1.5492e-02\n",
       " -6.4433e-03 -3.9881e-02 -1.4241e-02  ...  -1.7338e-02  6.7836e-03  1.5492e-02\n",
       "\n",
       "( 4  ,.,.) = \n",
       " -7.3740e-03 -2.9381e-02 -1.0295e-02  ...  -8.0469e-03  6.0250e-04 -1.1260e-03\n",
       " -1.5263e-02 -3.2167e-02 -1.7761e-02  ...  -1.0075e-02 -6.5915e-03 -1.7260e-04\n",
       " -5.0455e-02 -3.4033e-02 -2.1963e-02  ...  -1.2793e-03  1.8337e-02  4.9837e-03\n",
       "                 ...                   ⋱                   ...                \n",
       " -6.4433e-03 -3.9881e-02 -1.4241e-02  ...  -1.7338e-02  6.7836e-03  1.5492e-02\n",
       " -6.4433e-03 -3.9881e-02 -1.4241e-02  ...  -1.7338e-02  6.7836e-03  1.5492e-02\n",
       " -6.4433e-03 -3.9881e-02 -1.4241e-02  ...  -1.7338e-02  6.7836e-03  1.5492e-02\n",
       "[torch.FloatTensor of size 5x1640x300]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(features[:5].long(), seq_len[:5].long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
